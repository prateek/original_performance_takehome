# PRD: Original Performance Take‑Home (Local Optimization Plan)

## Objective

Minimize simulated cycle count for the kernel generated by
`KernelBuilder.build_kernel` (`perf_takehome.py`) while preserving correctness.

## Document Conventions

- `AGENTS.md` is the stable “how to work in this repo” guide (commands, repo map,
  invariants, integrity rules).
- `PRD.md` is the living “what to do next” plan + progress log that the
  automation loop (`./ralph-loop.sh`) follows.

## Constraints (Non‑Negotiable)

- **Do not modify anything under `tests/`.** Submissions that change the harness
  are invalid.
- Scoring uses the frozen simulator in `tests/frozen_problem.py`, so simulator
  changes in `problem.py` won’t count.
- Multicore is intentionally disabled: `N_CORES = 1`.

## How To Measure

- Authoritative: `python tests/submission_tests.py` (prints `CYCLES:`).
- Debug/trace loop:
  - `python perf_takehome.py Tests.test_kernel_trace`
  - `python watch_trace.py`

## Success Criteria

- Correctness: output values match the reference kernel for submission tests.
- Performance: improve over baseline (`147734` cycles) and track progress against
  the thresholds listed in `Readme.md` (e.g. `< 18532`, `< 2164`, `< 1487`).
- Stretch target: **200–500 cycles** (aggressive).

## Work Plan (Highest Priority First)

1. **VLIW packing:** improve `KernelBuilder.build` to pack independent slots into
   fewer instruction bundles while respecting `SLOT_LIMITS`.
2. **SIMD/vectorization:** use `vload`/`vstore` + `vbroadcast` + `valu` ops to
   process `VLEN=8` lanes at once where possible.
3. **Reduce memory traffic:** keep hot loop state in scratch; minimize
   load/store frequency to `mem` (only write back when required by correctness).
4. **Pipeline/unroll:** restructure the hash stages to reduce dependency chains
   and improve per‑cycle utilization.
5. **Documentation:** keep `AGENTS.md` and this PRD accurate for future runs.

## Current Status (2026-01-29)

- Current best is **`1378` cycles** on the authoritative harness (`python tests/submission_tests.py`).
- The remaining gap to the next submission threshold (`< 1363`) is **15 cycles**.

## Next Experiments (Ideas)

- Reduce load-engine pressure in gather rounds (depth ≥ 3) without increasing `valu` bottlenecks.
- Revisit depth-3 fast-path ideas (idx in `{7..14}`) only if selection can be done with very low `valu`/`flow` overhead.
- Use `python perf_takehome.py Tests.test_kernel_trace` + `watch_trace.py` to identify remaining bubbles in load/valu overlap before making larger scheduler changes.

## Repo Progress (Implementation)

- 2026-01-27: Added `AGENTS.md` (agent guide + repo integrity rules).
- 2026-01-28: Implemented greedy, hazard-aware VLIW slot packing in `KernelBuilder.build` and enabled it for the kernel body (`vliw=True`). Current cycles: `98583` (from `python tests/submission_tests.py`).
- 2026-01-28: Implemented SIMD vectorization over `VLEN=8` in `KernelBuilder.build_kernel` (`vload`/`vstore`, `valu` hash ops, gather via `load_offset`) and precomputed per-group input pointers. Current cycles: `12369` (from `python tests/submission_tests.py`).
- 2026-01-28: Reduced memory traffic by caching `inp_indices`/`inp_values` vectors in scratch (`idx_cache`/`val_cache`), eliminating per-round `vload`/`vstore` and only writing back once at the end. Current cycles: `11407` (from `python tests/submission_tests.py`).
- 2026-01-28: Implemented software-pipelined, group-interleaved scheduling in `KernelBuilder.build_kernel` to overlap gather loads with hash/idx updates (dynamic state machine; per-group `node_cache` + hash temps; uses `multiply_add` for `idx = idx*2 + step`). Current cycles: `2264` (from `python tests/submission_tests.py`).
- 2026-01-28: Improved VLIW utilization by moving `step`/`idx` masking vector ops from `valu` to `flow` (`vselect`) and packing scalar const loads + `vbroadcast` init with `build(vliw=True)`. Current cycles: `2159` (from `python tests/submission_tests.py`).
- 2026-01-28: Reduced gather memory traffic for rounds where indices are provably `0/1/2` (depth 0–1): load nodes `[0,1,2]` once, materialize `node0_vec`, `node2_vec`, and `node1-node2` vectors, and add per-group depth-aware states (`ROOT_XOR`, `DEPTH1_MASK`, `DEPTH1_NODE`) to skip `load_offset` gathers on those rounds; also reused `node_cache` as the primary hash temp to free scratch. Current cycles: `2098` (from `python tests/submission_tests.py`).
- 2026-01-28: Packed kernel prologue header loads (`mem[0..6]`) into VLIW bundles (preload `hdr_idxs` and issue batched `load` ops) to reduce init overhead. Current cycles: `2091` (from `python tests/submission_tests.py`).
- 2026-01-28: Reduced hash-stage instruction count by fusing the “add + shift-left + add” hash stages (depth-independent) into single `valu` `multiply_add` ops (e.g. `a + (a<<12) + C` → `a*4097 + C`). Current cycles: `1824` (from `python tests/submission_tests.py`).
- 2026-01-28: Reduced gather memory traffic for the depth-2 round (idx in `{3,4,5,6}`) by preloading nodes `[3..6]` once, materializing `node3_vec` + `node{4,5,6}-node3` delta vectors, and adding depth-aware scheduler states (`DEPTH2_*`) to avoid `load_offset` gathers on those rounds. Current cycles: `1672` (from `python tests/submission_tests.py`).
- 2026-01-28: Offloaded gather address prefetch from `valu` to scalar `alu` (8 adds) to free `valu` bandwidth for hash work; removed `forest_values_p_vec` broadcast. Current cycles: `1637` (from `python tests/submission_tests.py`).
- 2026-01-28: Removed per-round idx bounds-check/clamp (`idx < n_nodes` + `vselect`) by exploiting the deterministic depth schedule (starting idx=0, wrap only when `next_round % (forest_height+1) == 0`); added `IDX_RESET` flow state for the wrap round and skipped idx update entirely on the final round (values-only correctness). Current cycles: `1570` (from `python tests/submission_tests.py`).
- 2026-01-28: Removed index memory traffic by keeping indices entirely in scratch (inputs start at 0), eliminating `idx_ptrs` + index `vload`/`vstore`; trimmed unused header loads; folded `pause` into existing bundles to avoid standalone pause cycles. Current cycles: `1547` (from `python tests/submission_tests.py`).
- 2026-01-28: Reduced `valu` pressure on gather rounds by doing lane-wise XOR (`val ^= node`) via scalar `alu` slots overlapped with the gather-load pipeline (post-load XOR queue), avoiding the dedicated vector XOR step for those rounds. Current cycles: `1538` (from `python tests/submission_tests.py`).
- 2026-01-28: Re-ran authoritative submission tests to confirm the current best cycle count remains `1538` (`python tests/submission_tests.py`).
- 2026-01-28: Reduced prologue load overhead by preloading forest nodes `[0..7]` via a single `vload` and broadcasting from scratch for the depth 0–2 fast paths. Current cycles: `1529` (from `python tests/submission_tests.py`).
- 2026-01-28: Tried extending the fast-path node selection to the depth-3 round (idx in `{7..14}`) via preloading nodes `[7..14]` and masked-add selection, but it regressed cycles to `1636`, so it was reverted. Current best remains `1529` (from `python tests/submission_tests.py`).
- 2026-01-28: Smoothed software-pipeline fill/drain by staggering per-group start times (`ready[g] = g*2`), eliminating early load-engine bubbles (first gather starts earlier) and improving overlap. Current cycles: `1483` (from `python tests/submission_tests.py`).
- 2026-01-28: Reduced setup overhead by preloading `val_cache` + materializing `val_ptrs` with 4 independent pointer streams (blocks of 8 groups) instead of a 32-step dependency chain; retuned group start staggering (`start_spacing=14`). Current cycles: `1418` (from `python tests/submission_tests.py`).
- 2026-01-28: Improved VLIW packing by replacing the in-order greedy bundler in `KernelBuilder.build` with a dependency-aware list scheduler (tracks RAW/WAW + WAR constraints) so independent loads/ALU/valu ops can be reordered and packed more tightly. Current cycles: `1414` (from `python tests/submission_tests.py`).
- 2026-01-28: Further reduced setup overhead by packing scalar const loads, vector broadcasts, and the setup/caching phase into a single `build(vliw=True)` pass so load/alu/valu work overlaps. Current cycles: `1409` (from `python tests/submission_tests.py`).
- 2026-01-28: Retuned group start staggering in the main software-pipelined scheduler (piecewise offsets: first 4 groups start 4 cycles apart, then 16-cycle spacing) to reduce early valu underutilization without increasing gather contention. Current cycles: `1391` (from `python tests/submission_tests.py`).
- 2026-01-29: Pipelined `val_cache` initialization to reduce startup overhead: preload only the first two groups in setup, then stream the remaining groups’ `vload`s during early load-idle main-loop cycles (`INIT_LOAD` state). Current cycles: `1378` (from `python tests/submission_tests.py`).
- 2026-01-29: Updated `PRD.md` with current status + a short “Next Experiments” checklist. Current cycles remain `1378` (from `python tests/submission_tests.py`).
