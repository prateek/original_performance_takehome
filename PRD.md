# PRD: Original Performance Take‑Home (Local Optimization Plan)

## Objective

Minimize simulated cycle count for the kernel generated by
`KernelBuilder.build_kernel` (`perf_takehome.py`) while preserving correctness.

## Document Conventions

- `AGENTS.md` is the stable “how to work in this repo” guide (commands, repo map,
  invariants, integrity rules).
- `PRD.md` is the living “what to do next” plan + progress log that the
  automation loop (`./ralph-loop.sh`) follows.

## Constraints (Non‑Negotiable)

- **Do not modify anything under `tests/`.** Submissions that change the harness
  are invalid.
- Scoring uses the frozen simulator in `tests/frozen_problem.py`, so simulator
  changes in `problem.py` won’t count.
- Multicore is intentionally disabled: `N_CORES = 1`.

## How To Measure

- Authoritative: `python tests/submission_tests.py` (prints `CYCLES:`).
- Debug/trace loop:
  - `python perf_takehome.py Tests.test_kernel_trace`
  - `python watch_trace.py`

## Success Criteria

- Correctness: output values match the reference kernel for submission tests.
- Performance: improve over baseline (`147734` cycles) and track progress against
  the thresholds listed in `Readme.md` (e.g. `< 18532`, `< 2164`, `< 1487`).
- Stretch target: **200–500 cycles** (aggressive).

## Work Plan (Highest Priority First)

1. **VLIW packing:** improve `KernelBuilder.build` to pack independent slots into
   fewer instruction bundles while respecting `SLOT_LIMITS`.
2. **SIMD/vectorization:** use `vload`/`vstore` + `vbroadcast` + `valu` ops to
   process `VLEN=8` lanes at once where possible.
3. **Reduce memory traffic:** keep hot loop state in scratch; minimize
   load/store frequency to `mem` (only write back when required by correctness).
4. **Pipeline/unroll:** restructure the hash stages to reduce dependency chains
   and improve per‑cycle utilization.
5. **Documentation:** keep `AGENTS.md` and this PRD accurate for future runs.

## Repo Progress (Implementation)

- 2026-01-27: Added `AGENTS.md` (agent guide + repo integrity rules).
- 2026-01-28: Implemented greedy, hazard-aware VLIW slot packing in `KernelBuilder.build` and enabled it for the kernel body (`vliw=True`). Current cycles: `98583` (from `python tests/submission_tests.py`).
- 2026-01-28: Implemented SIMD vectorization over `VLEN=8` in `KernelBuilder.build_kernel` (`vload`/`vstore`, `valu` hash ops, gather via `load_offset`) and precomputed per-group input pointers. Current cycles: `12369` (from `python tests/submission_tests.py`).
- 2026-01-28: Reduced memory traffic by caching `inp_indices`/`inp_values` vectors in scratch (`idx_cache`/`val_cache`), eliminating per-round `vload`/`vstore` and only writing back once at the end. Current cycles: `11407` (from `python tests/submission_tests.py`).
- 2026-01-28: Implemented software-pipelined, group-interleaved scheduling in `KernelBuilder.build_kernel` to overlap gather loads with hash/idx updates (dynamic state machine; per-group `node_cache` + hash temps; uses `multiply_add` for `idx = idx*2 + step`). Current cycles: `2264` (from `python tests/submission_tests.py`).
- 2026-01-28: Improved VLIW utilization by moving `step`/`idx` masking vector ops from `valu` to `flow` (`vselect`) and packing scalar const loads + `vbroadcast` init with `build(vliw=True)`. Current cycles: `2159` (from `python tests/submission_tests.py`).
- 2026-01-28: Reduced gather memory traffic for rounds where indices are provably `0/1/2` (depth 0–1): load nodes `[0,1,2]` once, materialize `node0_vec`, `node2_vec`, and `node1-node2` vectors, and add per-group depth-aware states (`ROOT_XOR`, `DEPTH1_MASK`, `DEPTH1_NODE`) to skip `load_offset` gathers on those rounds; also reused `node_cache` as the primary hash temp to free scratch. Current cycles: `2098` (from `python tests/submission_tests.py`).
- 2026-01-28: Packed kernel prologue header loads (`mem[0..6]`) into VLIW bundles (preload `hdr_idxs` and issue batched `load` ops) to reduce init overhead. Current cycles: `2091` (from `python tests/submission_tests.py`).
- 2026-01-28: Reduced hash-stage instruction count by fusing the “add + shift-left + add” hash stages (depth-independent) into single `valu` `multiply_add` ops (e.g. `a + (a<<12) + C` → `a*4097 + C`). Current cycles: `1824` (from `python tests/submission_tests.py`).
- 2026-01-28: Reduced gather memory traffic for the depth-2 round (idx in `{3,4,5,6}`) by preloading nodes `[3..6]` once, materializing `node3_vec` + `node{4,5,6}-node3` delta vectors, and adding depth-aware scheduler states (`DEPTH2_*`) to avoid `load_offset` gathers on those rounds. Current cycles: `1672` (from `python tests/submission_tests.py`).
- 2026-01-28: Offloaded gather address prefetch from `valu` to scalar `alu` (8 adds) to free `valu` bandwidth for hash work; removed `forest_values_p_vec` broadcast. Current cycles: `1637` (from `python tests/submission_tests.py`).
- 2026-01-28: Removed per-round idx bounds-check/clamp (`idx < n_nodes` + `vselect`) by exploiting the deterministic depth schedule (starting idx=0, wrap only when `next_round % (forest_height+1) == 0`); added `IDX_RESET` flow state for the wrap round and skipped idx update entirely on the final round (values-only correctness). Current cycles: `1570` (from `python tests/submission_tests.py`).
- 2026-01-28: Removed index memory traffic by keeping indices entirely in scratch (inputs start at 0), eliminating `idx_ptrs` + index `vload`/`vstore`; trimmed unused header loads; folded `pause` into existing bundles to avoid standalone pause cycles. Current cycles: `1547` (from `python tests/submission_tests.py`).
